1 如果map的某个输出key value过大，会调用spillOneRecord，这时会出问题，需要先把当前缓冲区中的spill掉
2 压缩的输入，Recordeader没有提供压缩的位置，也提供不了，IndexRecord中的值设置有问题；无法判断一个map的输入split究竟有多少压缩后的字节
3 map task完成时，org.apache.hadoop.mapred.Task.done(Task.java:1026)会去检查map输出文件的大小，但是文件没有了，会跑异常org.apache.hadoop.util.DiskChecker$DiskErrorException
4 如果某个输入区间没有产生任何的输出，则spill会发生问题，不会向master报告已经处理该段输入
5 ShuffleSchedulerImpl::maxMapRunTime
6 MergeManagerImpl.java,createInMemorySegments:617，是否应该替换为spillInfo，感觉这里mapAttemptId和reduceAttemptId混淆了
7 需要删除调试日志
8 任务失败，暂时只考虑其所在节点没有宕机的情况
